{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nMNIST classification\n====================\n\nexample of image (MNIST) classification on small part of the data\nand with a small architecture\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import symjax.tensor as T\nfrom symjax import nn\nimport symjax\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom symjax.data import mnist\nfrom symjax.data.utils import batchify\n\nimport os\n\nos.environ[\"DATASET_PATH\"] = \"/home/vrael/DATASETS/\"\nsymjax.current_graph().reset()\n# load the dataset\nmnist = mnist.load()\n\n# some renormalization, and we only keep the first 2000 images\nmnist[\"train_set/images\"] = mnist[\"train_set/images\"][:2000]\nmnist[\"train_set/labels\"] = mnist[\"train_set/labels\"][:2000]\n\nmnist[\"train_set/images\"] /= mnist[\"train_set/images\"].max(\n    (1, 2, 3), keepdims=True\n)\nmnist[\"test_set/images\"] /= mnist[\"test_set/images\"].max(\n    (1, 2, 3), keepdims=True\n)\n\n# create the network\nBATCH_SIZE = 32\nimages = T.Placeholder((BATCH_SIZE, 1, 28, 28), \"float32\", name=\"images\")\nlabels = T.Placeholder((BATCH_SIZE,), \"int32\", name=\"labels\")\ndeterministic = T.Placeholder((1,), \"bool\")\n\n\nlayer = [nn.layers.Identity(images)]\n\nfor l in range(3):\n    layer.append(nn.layers.Conv2D(layer[-1], 32, (3, 3), b=None, pad=\"SAME\"))\n    layer.append(nn.layers.BatchNormalization(layer[-1], [1], deterministic))\n    layer.append(nn.leaky_relu(layer[-1]))\n    layer.append(nn.layers.Pool2D(layer[-1], (2, 2)))\n\nlayer.append(nn.layers.Pool2D(layer[-1], layer[-1].shape[2:], pool_type=\"AVG\"))\nlayer.append(nn.layers.Dense(layer[-1], 10))\n\n# each layer is itself a tensor which represents its output and thus\n# any tensor operation can be used on the layer instance, for example\nfor l in layer:\n    print(l.shape)\n\n\nloss = nn.losses.sparse_crossentropy_logits(labels, layer[-1]).mean()\naccuracy = nn.losses.accuracy(labels, layer[-1])\n\nnn.optimizers.Adam(loss, 0.01)\n\ntest = symjax.function(images, labels, deterministic, outputs=[loss, accuracy])\n\ntrain = symjax.function(\n    images,\n    labels,\n    deterministic,\n    outputs=[loss, accuracy],\n    updates=symjax.get_updates(),\n)\n\ntest_accuracy = []\n\nfor epoch in range(10):\n    L = list()\n    for x, y in batchify(\n        mnist[\"test_set/images\"],\n        mnist[\"test_set/labels\"],\n        batch_size=BATCH_SIZE,\n        option=\"continuous\",\n    ):\n        L.append(test(x, y, 1))\n    print(\"Test Loss and Accu:\", np.mean(L, 0))\n    test_accuracy.append(np.mean(L, 0))\n    L = list()\n    for x, y in batchify(\n        mnist[\"train_set/images\"],\n        mnist[\"train_set/labels\"],\n        batch_size=BATCH_SIZE,\n        option=\"random_see_all\",\n    ):\n        L.append(train(x, y, 0))\n    print(\"Train Loss and Accu\", np.mean(L, 0))\n\nplt.plot(test_accuracy)\nplt.xlabel(\"epochs\")\nplt.ylabel(\"accuracy\")\nplt.title(\"MNIST (1K data) classification task\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}