{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nRNN/GRU example\n===========\n\nexample of vanilla RNN for time series regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import symjax.tensor as T\nfrom symjax import nn\nimport symjax\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsymjax.current_graph().reset()\n# create the network\nBATCH_SIZE = 32\nTIME = 32\nWIDTH = 32\nC = 1\n\nnp.random.seed(0)\n\ntimeseries = T.Placeholder((BATCH_SIZE, TIME, C), \"float32\", name=\"time-series\")\ntarget = T.Placeholder((BATCH_SIZE, TIME), \"float32\", name=\"target\")\n\nrnn = nn.layers.RNN(timeseries, np.zeros((BATCH_SIZE, WIDTH)), WIDTH)\nrnn = nn.layers.RNN(rnn, np.zeros((BATCH_SIZE, WIDTH)), WIDTH)\nrnn = nn.layers.Dense(rnn, 1, flatten=False)\n\ngru = nn.layers.GRU(timeseries, np.zeros((BATCH_SIZE, WIDTH)), WIDTH)\ngru = nn.layers.GRU(gru, np.zeros((BATCH_SIZE, WIDTH)), WIDTH)\ngru = nn.layers.Dense(gru, 1, flatten=False)\n\nloss = ((target - rnn[:, :, 0]) ** 2).mean()\nlossg = ((target - gru[:, :, 0]) ** 2).mean()\n\nlr = nn.schedules.PiecewiseConstant(0.01, {1000: 0.005, 1800: 0.001})\n\nnn.optimizers.Adam(loss + lossg, lr)\n\n\ntrain = symjax.function(\n    timeseries, target, outputs=[loss, lossg], updates=symjax.get_updates(),\n)\n\npredict = symjax.function(timeseries, outputs=[rnn[:, :, 0], gru[:, :, 0]])\n\n\nx = [\n    np.random.randn(TIME) * 0.1 + np.cos(shift + np.linspace(-5, 10, TIME))\n    for shift in np.random.randn(BATCH_SIZE * 200) * 0.3\n]\nw = np.random.randn(TIME) * 0.01\ny = [(w + np.roll(xi, 2) * 0.4) ** 3 for xi in x]\ny = np.stack(y)\nx = np.stack(x)[:, :, None]\nx /= np.linalg.norm(x, 2, 1, keepdims=True)\nx -= x.min()\ny /= np.linalg.norm(y, 2, 1, keepdims=True)\n\n\nloss = []\nfor i in range(10):\n    for xb, yb in symjax.data.utils.batchify(x, y, batch_size=BATCH_SIZE):\n        loss.append(train(xb, yb))\n\nloss = np.stack(loss)\n\nplt.figure(figsize=(8, 8))\n\nplt.subplot(121)\nplt.plot(loss[:, 0], c=\"g\", label=\"Elman\")\nplt.plot(loss[:, 1], c=\"r\", label=\"GRU\")\nplt.title(\"Training loss\")\nplt.xlabel(\"Iterations\")\nplt.ylabel(\"MSE\")\nplt.legend()\n\npred = predict(x[:BATCH_SIZE])\n\nfor i in range(4):\n    plt.subplot(4, 2, 2 + 2 * i)\n\n    plt.plot(x[i, :, 0], \"-x\", c=\"k\", label=\"input\")\n    plt.plot(y[i], \"-x\", c=\"b\", label=\"target\")\n    plt.plot(pred[0][i], \"-x\", c=\"g\", label=\"Elman\")\n    plt.plot(pred[1][i], \"-x\", c=\"r\", label=\"GRU\")\n    plt.title(\"Predictions\")\n    plt.legend()\n\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}