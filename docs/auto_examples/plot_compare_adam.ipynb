{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nAdam TF and SymJAX\n==================\n\nIn this example we demonstrate how to perform a simple optimization with Adam in TF and SymJAX\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nimport symjax\nimport symjax.tensor as T\nfrom symjax.nn import optimizers\nimport numpy as np\nfrom tqdm import tqdm\n\n\nBS = 1000\nD = 500\nX = np.random.randn(BS, D).astype(\"float32\")\nY = X.dot(np.random.randn(D, 1).astype(\"float32\")) + 2\n\n\ndef TF1(x, y, N, lr, model, preallocate=False):\n    import tensorflow.compat.v1 as tf\n\n    tf.compat.v1.disable_v2_behavior()\n    tf.reset_default_graph()\n\n    tf_input = tf.placeholder(dtype=tf.float32, shape=[BS, D])\n    tf_output = tf.placeholder(dtype=tf.float32, shape=[BS, 1])\n\n    np.random.seed(0)\n\n    tf_W = tf.Variable(np.random.randn(D, 1).astype(\"float32\"))\n    tf_b = tf.Variable(\n        np.random.randn(\n            1,\n        ).astype(\"float32\")\n    )\n\n    tf_loss = tf.reduce_mean((tf.matmul(tf_input, tf_W) + tf_b - tf_output) ** 2)\n    if model == \"SGD\":\n        train_op = tf.train.GradientDescentOptimizer(lr).minimize(tf_loss)\n    elif model == \"Adam\":\n        train_op = tf.train.AdamOptimizer(lr).minimize(tf_loss)\n\n    # initialize session\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    sess.run(tf.global_variables_initializer())\n\n    losses = []\n    for i in tqdm(range(N)):\n        losses.append(\n            sess.run([tf_loss, train_op], feed_dict={tf_input: x, tf_output: y})[0]\n        )\n\n    return losses\n\n\ndef TF_EMA(X):\n    import tensorflow.compat.v1 as tf\n\n    tf.compat.v1.disable_v2_behavior()\n    tf.reset_default_graph()\n    x = tf.placeholder(\"float32\")\n    # Create an ExponentialMovingAverage object\n    ema = tf.train.ExponentialMovingAverage(decay=0.9)\n    op = ema.apply([x])\n    out = ema.average(x)\n\n    config = tf.ConfigProto()\n    config.gpu_options.allow_growth = True\n    sess = tf.Session(config=config)\n    sess.run(tf.global_variables_initializer(), feed_dict={x: X[0]})\n\n    outputs = []\n    for i in range(len(X)):\n        sess.run(op, feed_dict={x: X[i]})\n        outputs.append(sess.run(out))\n    return outputs\n\n\ndef SJ_EMA(X, debias=True):\n    symjax.current_graph().reset()\n    x = T.Placeholder((), \"float32\", name=\"x\")\n    value = symjax.nn.schedules.ExponentialMovingAverage(x, 0.9, debias=debias)[0]\n    print(x, value)\n    print(symjax.current_graph().roots(value))\n    train = symjax.function(x, outputs=value, updates=symjax.get_updates())\n    outputs = []\n    for i in range(len(X)):\n        outputs.append(train(X[i]))\n    return outputs\n\n\ndef SJ(x, y, N, lr, model, preallocate=False):\n    symjax.current_graph().reset()\n    sj_input = T.Placeholder(dtype=np.float32, shape=[BS, D])\n    sj_output = T.Placeholder(dtype=np.float32, shape=[BS, 1])\n\n    np.random.seed(0)\n\n    sj_W = T.Variable(np.random.randn(D, 1).astype(\"float32\"))\n    sj_b = T.Variable(\n        np.random.randn(\n            1,\n        ).astype(\"float32\")\n    )\n\n    sj_loss = ((sj_input.dot(sj_W) + sj_b - sj_output) ** 2).mean()\n\n    if model == \"SGD\":\n        optimizers.SGD(sj_loss, lr)\n    elif model == \"Adam\":\n        optimizers.Adam(sj_loss, lr)\n    train = symjax.function(\n        sj_input, sj_output, outputs=sj_loss, updates=symjax.get_updates()\n    )\n\n    losses = []\n    for i in tqdm(range(N)):\n        losses.append(train(x, y))\n\n    return losses\n\n\nsample = np.random.randn(100)\n\nplt.figure()\nplt.plot(sample, label=\"Original signal\", alpha=0.5)\nplt.plot(TF_EMA(sample), c=\"orange\", label=\"TF ema\", linewidth=2, alpha=0.5)\nplt.plot(SJ_EMA(sample), c=\"green\", label=\"SJ ema (biased)\", linewidth=2, alpha=0.5)\nplt.plot(\n    SJ_EMA(sample, False),\n    c=\"green\",\n    linestyle=\"--\",\n    label=\"SJ ema (unbiased)\",\n    linewidth=2,\n    alpha=0.5,\n)\nplt.legend()\n\n\nplt.figure()\nNs = [400, 1000]\nlrs = [0.001, 0.01, 0.1]\ncolors = [\"r\", \"b\", \"g\"]\nfor k, N in enumerate(Ns):\n    plt.subplot(1, len(Ns), 1 + k)\n    for c, lr in enumerate(lrs):\n        loss = TF1(X, Y, N, lr, \"Adam\")\n        plt.plot(loss, c=colors[c], linestyle=\"-\", alpha=0.5)\n        loss = SJ(X, Y, N, lr, \"Adam\")\n        plt.plot(loss, c=colors[c], linestyle=\"--\", alpha=0.5, linewidth=2)\n        plt.title(\"lr:\" + str(lr))\nplt.suptitle(\"Adam Optimization quadratic loss (-:TF, --:SJ)\")\n\n\nplt.figure()\nNs = [400, 1000]\nlrs = [0.001, 0.01, 0.1]\ncolors = [\"r\", \"b\", \"g\"]\nfor k, N in enumerate(Ns):\n    plt.subplot(1, len(Ns), 1 + k)\n    for c, lr in enumerate(lrs):\n        loss = TF1(X, Y, N, lr, \"SGD\")\n        plt.plot(loss, c=colors[c], linestyle=\"-\", alpha=0.5)\n        loss = SJ(X, Y, N, lr, \"SGD\")\n        plt.plot(loss, c=colors[c], linestyle=\"--\", alpha=0.5, linewidth=2)\n        plt.title(\"lr:\" + str(lr))\n        plt.xlabel(\"steps\")\nplt.suptitle(\"GD Optimization quadratic loss (-:TF, --:SJ)\")\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}