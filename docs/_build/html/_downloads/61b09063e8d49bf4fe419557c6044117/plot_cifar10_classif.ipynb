{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nCIFAR10 classification\n======================\n\nexample of image classification\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import symjax.tensor as T\nimport symjax as sj\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n# load the dataset\ncifar10 = sj.data.cifar10.load()\n\n# some renormalization\ncifar10['train_set/images'] /= cifar10['train_set/images'].max((1, 2, 3), keepdims=True)\ncifar10['test_set/images'] /= cifar10['test_set/images'].max((1, 2, 3), keepdims=True)\n\n# create the network\nBATCH_SIZE = 32\nimages = T.Placeholder((BATCH_SIZE,3, 32, 32), 'float32')\nlabels = T.Placeholder((BATCH_SIZE,), 'int32')\ndeterministic = T.Placeholder((1,), 'bool')\n\nlayer = [sj.layers.RandomCrop(images, crop_shape=(3, 32, 32),\n                padding=[(0, 0), (4, 4), (4, 4)],\n                deterministic=deterministic)]\n\nfor l in range(8):\n    layer.append(sj.layers.Conv2D(layer[-1], 32, (3, 3), b=None, pad='SAME'))\n    layer.append(sj.layers.BatchNormalization(layer[-1], [0, 2, 3],\n                                    deterministic))\n    layer.append(sj.layers.Lambda(layer[-1], T.leaky_relu))\n    if l % 3 == 0:\n        layer.append(sj.layers.Pool2D(layer[-1], (2, 2)))\n\nlayer.append(sj.layers.Pool2D(layer[-1], layer[-1].shape[2:], pool_type='AVG'))\n\nlayer.append(sj.layers.Dense(layer[-1], 10))\n\n# each layer is itself a tensor which represents its output and thus\n# any tensor operation can be used on the layer instance, for example\nfor l in layer:\n    print(l.shape)\n\n\nloss = sj.losses.sparse_crossentropy_logits(labels, layer[-1]).mean()\naccuracy = sj.losses.accuracy(labels, layer[-1])\n\nlr=sj.schedules.PiecewiseConstant(0.01, {15: 0.001, 25: 0.0001})\nopt = sj.optimizers.Adam(loss, lr)\n\nnetwork_updates = sj.layers.get_updates(layer)\n\ntest = sj.function(images, labels, deterministic, outputs=[loss, accuracy])\n\ntrain = sj.function(images, labels, deterministic,\n                    outputs=[loss, accuracy], updates={**opt.updates,\n                                                **network_updates})\n\ntest_accuracy = []\n\nfor epoch in range(3):\n    L = list()\n    for x, y in sj.data.batchify(cifar10['test_set/images'], cifar10['test_set/labels'], batch_size=BATCH_SIZE,\n                                      option='continuous'):\n        L.append(test(x, y, 1))\n    print('Test Loss and Accu:', np.mean(L, 0))\n    test_accuracy.append(np.mean(L, 0))\n    L = list()\n    for x, y in sj.data.batchify(cifar10['train_set/images'], cifar10['train_set/labels'],\n                            batch_size=BATCH_SIZE, option='random_see_all'):\n        L.append(train(x, y, 0))\n    print('Train Loss and Accu', np.mean(L, 0))\n    lr.update()\n\nplt.plot(test_accuracy)\nplt.xlabel('epochs')\nplt.ylabel('accuracy')\nplt.title('CIFAR10 classification task')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}