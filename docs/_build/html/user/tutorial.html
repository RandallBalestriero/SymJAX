

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Tutorial &mdash; symjax  documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
        <script src="../_static/jquery.js"></script>
        <script src="../_static/underscore.js"></script>
        <script src="../_static/doctools.js"></script>
        <script src="../_static/language_data.js"></script>
        <script async="async" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-binder.css" type="text/css" />
  <link rel="stylesheet" href="../_static/gallery-dataframe.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Gallery" href="../auto_examples/index.html" />
    <link rel="prev" title="Installation" href="installation.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../index.html">
          

          
            
            <img src="../_static/symjax_logo.png" class="logo" alt="Logo"/>
          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Tutorial</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#function-compiling-a-graph-into-an-executable-function">Function: compiling a graph into an executable (function)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#while-map-scan">While/Map/Scan</a></li>
<li class="toctree-l2"><a class="reference internal" href="#variable-batch-length-shape">Variable batch length (shape)</a></li>
<li class="toctree-l2"><a class="reference internal" href="#graph-visualization">Graph visualization</a></li>
<li class="toctree-l2"><a class="reference internal" href="#clone-one-line-multipurpose-graph-replacement">Clone: one line multipurpose graph replacement</a></li>
<li class="toctree-l2"><a class="reference internal" href="#scopes-operations-variables-placeholders-naming-and-accessing">Scopes, Operations/Variables/Placeholders naming and accessing</a></li>
<li class="toctree-l2"><a class="reference internal" href="#graph-saving-and-loading">Graph Saving and Loading</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wrap-jax-function-computation-to-symjax-op">Wrap: Jax function/computation to SymJAX Op</a></li>
<li class="toctree-l2"><a class="reference internal" href="#wrap-jax-class-to-symjax-class">Wrap: Jax class to SymJAX class</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../auto_examples/index.html">Gallery</a></li>
<li class="toctree-l1"><a class="reference internal" href="developers.html">Development</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../modules/symjax.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/data.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.data</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/tensor.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.tensor</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/interpolation.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.tensor.interpolation</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/signal.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.tensor.signal</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/fft.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.tensor.fft</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/random.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.tensor.random</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/nn.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.nn</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/initializers.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.nn.initializers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/layers.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.nn.layers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/optimizers.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.nn.optimizers</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/schedules.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.nn.schedules</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/losses.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.nn.losses</span></code></a></li>
<li class="toctree-l1"><a class="reference internal" href="../modules/probabilities.html"><code class="xref py py-mod docutils literal notranslate"><span class="pre">symjax.probabilities</span></code></a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../index.html">symjax</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../index.html">Docs</a> &raquo;</li>
        
      <li>Tutorial</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../_sources/user/tutorial.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="tutorial">
<h1>Tutorial<a class="headerlink" href="#tutorial" title="Permalink to this headline">¶</a></h1>
<p>We briefly describe some key components of SymJAX.</p>
<div class="section" id="function-compiling-a-graph-into-an-executable-function">
<span id="function"></span><h2>Function: compiling a graph into an executable (function)<a class="headerlink" href="#function-compiling-a-graph-into-an-executable-function" title="Permalink to this headline">¶</a></h2>
<p>As opposed to most current softwares, SymJAX proposes a symbolic viewpoint
from which one can create a computational graph, laying out all the computation
pipeline from inputs to outputs including updates of persistent variables. Once
this is defined, it is possible to compile this graph to optimize the exection
speed. In fact, knowing the graph (nodes, connections, shapes, types, constant
values) is enough to produce an highly optimized executable of this graph. In
SymJAX this is done via  symjax.function as demonstrated below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">symjax</span>
<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>



<span class="n">value</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ones</span><span class="p">(()))</span>
<span class="n">randn</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(())</span>
<span class="n">rand</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(())</span>

<span class="n">out1</span> <span class="o">=</span> <span class="n">randn</span> <span class="o">*</span> <span class="n">value</span>
<span class="n">out2</span> <span class="o">=</span> <span class="n">out1</span><span class="o">.</span><span class="n">clone</span><span class="p">({</span><span class="n">randn</span><span class="p">:</span> <span class="n">rand</span><span class="p">})</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">rand</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out2</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="p">{</span><span class="n">value</span><span class="p">:</span><span class="mi">2</span><span class="o">+</span><span class="n">value</span><span class="p">})</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">3</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">i</span><span class="p">))</span>
<span class="c1"># 0.</span>
<span class="c1"># 3.</span>
<span class="c1"># 10.</span>


<span class="c1"># we create a simple computational graph</span>
<span class="n">var</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span> <span class="n">seed</span><span class="o">=</span><span class="mi">10</span><span class="p">))</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">var</span> <span class="o">-</span> <span class="n">T</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">var</span><span class="p">))</span><span class="o">**</span><span class="mi">2</span> <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">var</span><span class="p">])</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="mf">0.01</span><span class="p">,</span> <span class="n">params</span><span class="o">=</span><span class="n">var</span><span class="p">)</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="n">opt</span><span class="o">.</span><span class="n">updates</span><span class="p">)</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
<span class="c1"># 240.96829</span>
<span class="c1"># 231.42595</span>
<span class="c1"># 222.26149</span>
<span class="c1"># 213.45993</span>
<span class="c1"># 205.00691</span>
<span class="c1"># 196.88864</span>
<span class="c1"># 189.09186</span>
<span class="c1"># 181.60382</span>
<span class="c1"># 174.41231</span>
<span class="c1"># 167.50558</span>


</pre></div>
</div>
</div>
<div class="section" id="while-map-scan">
<h2>While/Map/Scan<a class="headerlink" href="#while-map-scan" title="Permalink to this headline">¶</a></h2>
<p>An important part of many implementations resides in the use of for/while loops
and in scans, which allow to maintain and update an additional quantity through
the iterations. In SymJAX, those operators are different from the Jax ones and
closers to the Theano ones as they provide an explicit <code class="docutils literal notranslate"><span class="pre">sequences</span></code> and
<code class="docutils literal notranslate"><span class="pre">non_sequences</span></code> argument. Here are a few examples below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">symjax</span> <span class="k">as</span> <span class="nn">sj</span>
<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">u</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((),</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="p">(</span><span class="n">u</span> <span class="o">-</span> <span class="n">w</span><span class="p">)</span> <span class="o">*</span> <span class="n">a</span><span class="p">,</span> <span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)],</span> <span class="n">non_sequences</span><span class="o">=</span><span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">w</span> <span class="o">+</span> <span class="mi">1</span><span class="p">})</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># [0, 1, 2]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># [0, 0, 0]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="c1"># [0, -3, -6]</span>


<span class="n">w</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="n">w</span> <span class="o">*</span> <span class="n">a</span> <span class="o">*</span> <span class="n">u</span><span class="p">,</span> <span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)],</span> <span class="n">non_sequences</span><span class="o">=</span><span class="p">[</span><span class="n">w</span><span class="p">,</span> <span class="n">u</span><span class="p">])</span>
<span class="n">g</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">[</span><span class="n">w</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">u</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">g</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="c1"># 0</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 3</span>


<span class="n">out</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">:</span> <span class="n">a</span> <span class="o">*</span> <span class="n">b</span><span class="p">,</span> <span class="p">[</span><span class="n">T</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">),</span> <span class="n">T</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="mi">3</span><span class="p">)])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
<span class="c1"># [0, 1, 4]</span>


<span class="n">w</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="n">v</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((),</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">while_loop</span><span class="p">(</span><span class="k">lambda</span> <span class="n">i</span><span class="p">,</span> <span class="n">u</span><span class="p">:</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="n">u</span> <span class="o">&lt;</span> <span class="mi">5</span><span class="p">,</span>
                   <span class="k">lambda</span> <span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mf">1.</span><span class="p">,</span> <span class="n">i</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="n">w</span><span class="p">,</span> <span class="mf">1.</span><span class="p">),</span>
                   <span class="n">non_sequences_cond</span><span class="o">=</span><span class="p">[</span><span class="n">v</span><span class="p">])</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span>
<span class="c1"># 5, 16</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
<span class="c1"># [3, 4]</span>
</pre></div>
</div>
<p>the use of the <code class="docutils literal notranslate"><span class="pre">non_sequences</span></code> argument allows to keep track of the internal
function dependencies without requiring to execute the function. Hence all
tensors used inside a function should be part of the <code class="docutils literal notranslate"><span class="pre">sequences</span></code> or
<code class="docutils literal notranslate"><span class="pre">non_sequences</span></code> op inputs.</p>
</div>
<div class="section" id="variable-batch-length-shape">
<span id="none"></span><h2>Variable batch length (shape)<a class="headerlink" href="#variable-batch-length-shape" title="Permalink to this headline">¶</a></h2>
<p>In many applications it is required to have length varying inputs to a compiled
SymJAX function. This can be done by expliciting setting the shape of the corresponding
<code class="docutils literal notranslate"><span class="pre">Placeholders</span></code> to 0 (this will likely change in the future) as demonstrated
below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">symjax</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="mf">1.</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">p</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">sum</span><span class="p">()})</span>

<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">w</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
<span class="c1"># [2.]</span>
<span class="c1"># 2.0</span>
<span class="c1"># [2. 2.]</span>
<span class="c1"># 4.0</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((</span><span class="mi">0</span><span class="p">,),</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">w</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">Variable</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="s1">&#39;float32&#39;</span><span class="p">)</span>

<span class="n">loss</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="p">[</span><span class="n">w</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>

<span class="n">other_g</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">gradients</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(),</span> <span class="p">[</span><span class="n">w</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span> <span class="n">updates</span><span class="o">=</span><span class="p">{</span><span class="n">w</span><span class="p">:</span> <span class="n">w</span> <span class="o">-</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">g</span><span class="p">})</span>
<span class="n">other_f</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="n">other_g</span><span class="p">)</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)),</span> <span class="o">-</span><span class="mi">1</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)))</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">other_f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))))</span>

<span class="c1"># 9.0</span>
<span class="c1"># [1. 1.]</span>
<span class="c1"># 3.2399998</span>
<span class="c1"># [2. 2.]</span>
<span class="c1"># 1.1663998</span>
<span class="c1"># [3. 3.]</span>
<span class="c1"># 0.419904</span>
<span class="c1"># [4. 4.]</span>
<span class="c1"># 0.15116541</span>
<span class="c1"># [5. 5.]</span>
<span class="c1"># 0.05441956</span>
<span class="c1"># [6. 6.]</span>
<span class="c1"># 0.019591037</span>
<span class="c1"># [7. 7.]</span>
<span class="c1"># 0.007052775</span>
<span class="c1"># [8. 8.]</span>
<span class="c1"># 0.0025389965</span>
<span class="c1"># [9. 9.]</span>
<span class="c1"># 0.0009140394</span>
<span class="c1"># [10. 10.]</span>
</pre></div>
</div>
<p>in the backend, SymJAX automatically jit the overall (vmapped) functions for
optimal performances.</p>
</div>
<div class="section" id="graph-visualization">
<span id="viz"></span><h2>Graph visualization<a class="headerlink" href="#graph-visualization" title="Permalink to this headline">¶</a></h2>
<p>Similarly to Theano, it is possible to display the computational graph of the code written as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>

<span class="n">__author__</span>      <span class="o">=</span> <span class="s2">&quot;Randall Balestriero&quot;</span>

<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">from</span> <span class="nn">symjax.viz</span> <span class="k">import</span> <span class="n">compute_graph</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">z</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">10</span><span class="p">,),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;z&#39;</span><span class="p">)</span>

<span class="n">w</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>
<span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="o">+</span><span class="n">y</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">*</span> <span class="n">w</span> <span class="o">+</span> <span class="n">z</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>

<span class="n">graph</span> <span class="o">=</span> <span class="n">compute_graph</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
<span class="n">graph</span><span class="o">.</span><span class="n">draw</span><span class="p">(</span><span class="s1">&#39;file.png&#39;</span><span class="p">,</span><span class="n">prog</span><span class="o">=</span><span class="s1">&#39;dot&#39;</span><span class="p">)</span>


<span class="kn">import</span> <span class="nn">matplotlib.image</span> <span class="k">as</span> <span class="nn">mpimg</span>
<span class="n">img</span><span class="o">=</span><span class="n">mpimg</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="s1">&#39;file.png&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">imgplot</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xticks</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">yticks</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>

</pre></div>
</div>
<img alt="../_images/file.png" src="../_images/file.png" />
</div>
<div class="section" id="clone-one-line-multipurpose-graph-replacement">
<span id="clone"></span><h2>Clone: one line multipurpose graph replacement<a class="headerlink" href="#clone-one-line-multipurpose-graph-replacement" title="Permalink to this headline">¶</a></h2>
<p>In most current packages, the ability to perform an already define computation
graph but with altered nodes is cumbersone. Some specific involve the use
of layers as in Keras where one can feed any value hence allow to compute a
feedforward pass without much changes but if one had to replace a specific
variable or more complex part of a graph no tools are available. In Theano,
the clone function allowed to do such thing and it implemented in SymJAX as
well. As per the below example, it is clear how the clone utility allows to
get an already defined computational graph and replace any subgraph in it with
another based on a node-&gt;node mapping:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">symjax</span>
<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>

<span class="c1"># we create a simple mapping with 2 matrix multiplications interleaved</span>
<span class="c1"># with nonlinearities</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Placeholder</span><span class="p">((</span><span class="mi">8</span><span class="p">,),</span> <span class="s1">&#39;float32&#39;</span><span class="p">)</span>
<span class="n">w_1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">16</span><span class="p">,</span> <span class="mi">8</span><span class="p">)))</span>
<span class="n">w_2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">2</span><span class="p">,</span> <span class="mi">16</span><span class="p">)))</span>

<span class="c1"># the output can be computed easily as</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">w_2</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">w_1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">)))</span>

<span class="c1"># now suppose we also wanted the same mapping but with a noise input</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">8</span><span class="p">,))</span>

<span class="n">output_noisy</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">clone</span><span class="p">({</span><span class="n">x</span><span class="p">:</span><span class="n">x</span><span class="o">+</span><span class="n">epsilon</span><span class="p">})</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">,</span> <span class="n">output_noisy</span><span class="p">])</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">8</span><span class="p">)))</span>

<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-11.590391 ,   4.7543654], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-30.038504,  26.758451], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-19.214798,  19.600328], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-12.927457,  10.457445], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-19.486668,  17.367273], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-31.634314,  24.837488], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-19.756075,  12.330083], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-38.9738  ,  31.588022], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-19.561726,  12.192366], dtype=float32)]</span>
<span class="c1"># [array([-14.496595,   8.7136  ], dtype=float32), array([-33.110832,  30.104563], dtype=float32)]</span>

</pre></div>
</div>
</div>
<div class="section" id="scopes-operations-variables-placeholders-naming-and-accessing">
<span id="scopes"></span><h2>Scopes, Operations/Variables/Placeholders naming and accessing<a class="headerlink" href="#scopes-operations-variables-placeholders-naming-and-accessing" title="Permalink to this headline">¶</a></h2>
<p>Accessing, naming variables, operations and placeholders. This is done in a
similar way as in the vanilla Tensorflow form with scopes and EVERY of the
variable/placeholder/operation is named and located with a unique identifier
(name) per scope. If during creation both have same names, the original name is
augmented with an underscore and interger number, here is a brief example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">symjax</span>
<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>

<span class="c1"># scope/graph naming and accessing</span>

<span class="n">value1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>
<span class="n">value2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="s1">&#39;special&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">g</span><span class="p">:</span>
    <span class="n">value3</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>
    <span class="n">value4</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">value3</span> <span class="o">+</span> <span class="n">value4</span>

    <span class="n">h</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="s1">&#39;inversion&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">h</span><span class="p">:</span>
        <span class="n">value5</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="n">value6</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>
        <span class="n">value7</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;w&#39;</span><span class="p">)</span>


<span class="nb">print</span><span class="p">(</span><span class="n">g</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
<span class="c1"># {&#39;unnamed_variable&#39;: Variable(name=unnamed_variable, shape=(1,), dtype=float32, trainable=True, scope=/special/),</span>
<span class="c1">#  &#39;unnamed_variable_1&#39;: Variable(name=unnamed_variable_1, shape=(1,), dtype=float32, trainable=True, scope=/special/)}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">variables</span><span class="p">)</span>
<span class="c1">#{&#39;unnamed_variable&#39;: Variable(name=unnamed_variable, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/),</span>
<span class="c1"># &#39;unnamed_variable_1&#39;: Variable(name=unnamed_variable_1, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/),</span>
<span class="c1"># &#39;w&#39;: Variable(name=w, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/)}</span>

<span class="nb">print</span><span class="p">(</span><span class="n">h</span><span class="o">.</span><span class="n">variable</span><span class="p">(</span><span class="s1">&#39;w&#39;</span><span class="p">))</span>
<span class="c1"># Variable(name=w, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/)</span>

<span class="c1"># now suppose that we did not hold the value for the graph g/h, we can still</span>
<span class="c1"># recover a variable based on the name AND the scope</span>

<span class="nb">print</span><span class="p">(</span><span class="n">symjax</span><span class="o">.</span><span class="n">get_variables</span><span class="p">(</span><span class="s1">&#39;/special/inversion/w&#39;</span><span class="p">))</span>
<span class="c1"># Variable(name=w, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/)</span>

<span class="c1"># now if the exact scope name is not know, it is possible to use smart indexing</span>
<span class="c1"># for example suppose we do not remember, then we can get all variables named</span>
<span class="c1"># &#39;w&#39; among scopes</span>

<span class="nb">print</span><span class="p">(</span><span class="n">symjax</span><span class="o">.</span><span class="n">get_variables</span><span class="p">(</span><span class="s1">&#39;*/w&#39;</span><span class="p">))</span>
<span class="c1"># Variable(name=w, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/)</span>

<span class="c1"># if only part of the scope is known, all the variables of a given scope can</span>
<span class="c1"># be retreived</span>

<span class="nb">print</span><span class="p">(</span><span class="n">symjax</span><span class="o">.</span><span class="n">get_variables</span><span class="p">(</span><span class="s1">&#39;/special/*&#39;</span><span class="p">))</span>
<span class="c1"># [Variable(name=unnamed_variable, shape=(1,), dtype=float32, trainable=True, scope=/special/),</span>
<span class="c1">#  Variable(name=unnamed_variable_1, shape=(1,), dtype=float32, trainable=True, scope=/special/),</span>
<span class="c1">#  Variable(name=unnamed_variable, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/),</span>
<span class="c1">#  Variable(name=unnamed_variable_1, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/),</span>
<span class="c1">#  Variable(name=w, shape=(1,), dtype=float32, trainable=True, scope=/special/inversion/)]</span>

<span class="nb">print</span><span class="p">(</span><span class="n">symjax</span><span class="o">.</span><span class="n">get_ops</span><span class="p">(</span><span class="s1">&#39;*add&#39;</span><span class="p">))</span>
<span class="c1"># Op(name=add, shape=(1,), dtype=float32, scope=/special/)</span>
</pre></div>
</div>
</div>
<div class="section" id="graph-saving-and-loading">
<span id="saving"></span><h2>Graph Saving and Loading<a class="headerlink" href="#graph-saving-and-loading" title="Permalink to this headline">¶</a></h2>
<p>An important feature of SymJAX is the easiness to reset, save, load variables.
This is crucial in order to save a model and being to reloaded (in a possibly
different script) to keep using it. In our case, a computational graph is
completely defined by its structure and the values of the persistent nodes
(the variables). Hence, it is enough to save the variables. This is done in a
very explicit manner using the numpy.savez utility where the saved file can
be accessed from any other script, variables can be loaded, accessed, even
modified, and then reloaded inside the computational graph. Here is a brief
example:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">symjax</span>
<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>

<span class="n">g</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="s1">&#39;model1&#39;</span><span class="p">)</span>
<span class="k">with</span> <span class="n">g</span><span class="p">:</span>
    <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">1</span><span class="p">,)))</span>
    <span class="k">with</span> <span class="n">symjax</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="s1">&#39;layer1&#39;</span><span class="p">):</span>
        <span class="n">W1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
        <span class="n">b1</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">symjax</span><span class="o">.</span><span class="n">Graph</span><span class="p">(</span><span class="s1">&#39;layer2&#39;</span><span class="p">):</span>
        <span class="n">W2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;W&#39;</span><span class="p">)</span>
        <span class="n">b2</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">Variable</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,)),</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>

<span class="c1"># define an irrelevant loss function involving the parameters</span>
<span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">W1</span> <span class="o">+</span> <span class="n">b1</span> <span class="o">+</span> <span class="n">W2</span> <span class="o">+</span> <span class="n">b2</span><span class="p">)</span> <span class="o">*</span> <span class="n">learning_rate</span>

<span class="c1"># and a train/update function</span>
<span class="n">train</span> <span class="o">=</span> <span class="n">symjax</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">loss</span><span class="p">,</span>
                        <span class="n">updates</span><span class="o">=</span><span class="p">{</span><span class="n">W1</span><span class="p">:</span> <span class="n">W1</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">b1</span><span class="p">:</span> <span class="n">b1</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span> <span class="n">W2</span><span class="p">:</span> <span class="n">W2</span> <span class="o">+</span> <span class="mi">2</span><span class="p">,</span>
                                 <span class="n">b2</span><span class="p">:</span> <span class="n">b2</span> <span class="o">+</span> <span class="mi">3</span><span class="p">})</span>

<span class="c1"># pretend we train for a while</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="p">())</span>

<span class="c1"># [0.]</span>
<span class="c1"># [8.]</span>
<span class="c1"># [16.]</span>
<span class="c1"># [24.]</span>

<span class="c1"># now say we wanted to reset the variables and retrain, we can do</span>
<span class="c1"># either with g, as it contains all the variables</span>
<span class="n">g</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="c1"># or we can do</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">reset_variables</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
<span class="c1"># or if we wanted to only reset say variables from layer2</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">reset_variables</span><span class="p">(</span><span class="s1">&#39;*layer2*&#39;</span><span class="p">)</span>

<span class="c1"># now that all has been reset, let&#39;s retrain for a while</span>
<span class="c1"># pretend we train for a while</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="p">())</span>

<span class="c1"># [0.]</span>
<span class="c1"># [8.]</span>

<span class="c1"># now resetting is nice, but we might want to save the model parameters, to</span>
<span class="c1"># keep training later or do some other analyses. We can do so as follows:</span>
<span class="n">g</span><span class="o">.</span><span class="n">save_variables</span><span class="p">(</span><span class="s1">&#39;model1_saved&#39;</span><span class="p">)</span>
<span class="c1"># this would save all variables as they are contained in g. Now say we want to</span>
<span class="c1"># only save the second layer variables, if we had saved the graph variables as</span>
<span class="c1"># say h we could just do ``h.save(&#39;layer1_saved&#39;)&#39;&#39;</span>
<span class="c1"># but while we do not have it, we recall the scope of it, we can thus do</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">save_variables</span><span class="p">(</span><span class="s1">&#39;*layer1*&#39;</span><span class="p">,</span> <span class="s1">&#39;layer1_saved&#39;</span><span class="p">)</span>
<span class="c1"># and for the entire set of variables just do</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">save_variables</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="s1">&#39;model1_saved&#39;</span><span class="p">)</span>

<span class="c1"># now suppose that after training or after resetting</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">reset_variables</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>

<span class="c1"># one wants to recover the saved weights, one can do</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">load_variables</span><span class="p">(</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="s1">&#39;model1_saved&#39;</span><span class="p">)</span>
<span class="c1"># in that case all variables will be reloaded as they were in model1_saved,</span>
<span class="c1"># if we used symjax.load(&#39;*&#39;, &#39;layer1_saved&#39;), an error would occur as not all</span>
<span class="c1"># variables are present in this file, one should instead do</span>
<span class="c1"># (in this case, this is redundant as we loaded everything up above)</span>
<span class="n">symjax</span><span class="o">.</span><span class="n">load_variables</span><span class="p">(</span><span class="s1">&#39;*layer1*&#39;</span><span class="p">,</span> <span class="s1">&#39;layer1_saved&#39;</span><span class="p">)</span>

<span class="c1"># we can now pretend to keep training our model form its saved state</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">train</span><span class="p">())</span>

<span class="c1"># [16.]</span>
<span class="c1"># [24.]</span>
</pre></div>
</div>
</div>
<div class="section" id="wrap-jax-function-computation-to-symjax-op">
<span id="wrapf"></span><h2>Wrap: Jax function/computation to SymJAX Op<a class="headerlink" href="#wrap-jax-function-computation-to-symjax-op" title="Permalink to this headline">¶</a></h2>
<p>The computation in Jax is done eagerly similarly to TF2 and PyTorch. In SymJAX
the computational graph definition is done a priori with symbolic variables.
That is, no actual computations are done during the graph definition, once done
the graph is compiled with proper inputs/outputs/updates to provide the user
with a compiled function executing the graph. This graph thus involves various
operations, one can define its own in the two following way. First by combining
the already existing SymJAX function, the other by creating it in pure Jax and
then wrapping it into a SymJAX symbolic operation as demonstrated below.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">symjax</span> <span class="k">as</span> <span class="nn">sj</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="n">__author__</span>      <span class="o">=</span> <span class="s2">&quot;Randall Balestriero&quot;</span>

<span class="c1"># suppose we want to compute the mean-squared error between two vectors</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">((</span><span class="mi">10</span><span class="p">,))</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,))</span>

<span class="c1"># one way is to do so by combining SymJAX functions as</span>
<span class="n">mse</span> <span class="o">=</span> <span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span>
<span class="c1"># notice that the basic operators are overloaded and implicitly call SymJAX ops</span>

<span class="c1"># another solution is to create a new SymJAX Op from a jax computation as</span>
<span class="c1"># follows</span>

<span class="k">def</span> <span class="nf">mse_jax</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">sum</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">y</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># wrap the jax computation into a SymJAX Op that can then be used as any</span>
<span class="c1"># SymJAX function</span>
<span class="n">mse_op</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">tensor</span><span class="o">.</span><span class="n">jax_wrap</span><span class="p">(</span><span class="n">mse_jax</span><span class="p">)</span>
<span class="n">also_mse</span> <span class="o">=</span> <span class="n">mse_op</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">also_mse</span><span class="p">)</span>
<span class="c1"># Tensor(Op=mse_jax, shape=(), dtype=float32)</span>



<span class="c1"># ensure that both are equivalent</span>
<span class="n">f</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">mse</span><span class="p">,</span> <span class="n">also_mse</span><span class="p">])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="p">())</span>
<span class="c1"># [array(6.0395503, dtype=float32), array(6.0395503, dtype=float32)]</span>

</pre></div>
</div>
<p>A SymJAX computation graph can not be partially defined with Jax computation,
the above thus provides an easy way to wrap Jax computations into a SymJAX Op
which can then be put into the graph as any other SymJAX provided Ops.</p>
</div>
<div class="section" id="wrap-jax-class-to-symjax-class">
<span id="wrapc"></span><h2>Wrap: Jax class to SymJAX class<a class="headerlink" href="#wrap-jax-class-to-symjax-class" title="Permalink to this headline">¶</a></h2>
<p>One might have defined a Jax class, with a constructor possibly taking some
constant values and some jax arrays, performing some computations, setting
some attributes, and then interacting with those attributes when calling the
class methods. It would be particularly easy to pair such already implemented
classes with SymJAX computation graph. This can be done as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="ch">#!/usr/bin/env python</span>
<span class="c1"># -*- coding: utf-8 -*-</span>

<span class="kn">import</span> <span class="nn">symjax</span> <span class="k">as</span> <span class="nn">sj</span>
<span class="kn">import</span> <span class="nn">symjax.tensor</span> <span class="k">as</span> <span class="nn">T</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>

<span class="n">__author__</span>      <span class="o">=</span> <span class="s2">&quot;Randall Balestriero&quot;</span>

<span class="k">class</span> <span class="nc">product</span><span class="p">:</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">V</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">square</span><span class="p">(</span><span class="n">V</span> <span class="o">*</span> <span class="n">W</span> <span class="o">*</span> <span class="p">(</span><span class="n">W</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;float32&#39;</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">ndim</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">compute_ndim</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">feed</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">,</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">compute_ndim</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>



<span class="n">wrapped</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">wrap_class</span><span class="p">(</span><span class="n">product</span><span class="p">,</span> <span class="n">method_exceptions</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;compute_ndim&#39;</span><span class="p">])</span>


<span class="n">a</span> <span class="o">=</span> <span class="n">wrapped</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">10</span><span class="p">)),</span> <span class="n">V</span><span class="o">=</span><span class="n">T</span><span class="o">.</span><span class="n">ones</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)))</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">((</span><span class="mi">10</span><span class="p">,</span> <span class="mi">100</span><span class="p">))</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
<span class="c1"># (Tensor: name=function[0], shape=(10, 10), dtype=float32)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">a</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>
<span class="c1"># Op(name=feed, shape=(10, 100), dtype=float32, scope=/)</span>

<span class="n">f</span> <span class="o">=</span> <span class="n">sj</span><span class="o">.</span><span class="n">function</span><span class="p">(</span><span class="n">outputs</span><span class="o">=</span><span class="n">a</span><span class="o">.</span><span class="n">feed</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

<span class="n">f</span><span class="p">()</span>

</pre></div>
</div>
<p>As can be seen, there is some restrictions. First, the behavior inside the
constructor of the original class should be fixed as it will be executed once
by the wrapper in order to map the constructor computations into SymJAX.
Second, any jax array update done internally will break the conversion as
such operations are only allowed for Variables in SymJAX, hence some care is
needed. More flexibility will be provided in future versions.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../auto_examples/index.html" class="btn btn-neutral float-right" title="Gallery" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="installation.html" class="btn btn-neutral float-left" title="Installation" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2020, Randall Balestriero

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>